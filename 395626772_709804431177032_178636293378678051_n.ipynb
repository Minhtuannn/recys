{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a84e4e5-4450-49fe-a507-8496107d5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "TAU = 0.001\n",
    "BATCH_SIZE = 64\n",
    "N_HIDDEN_1 = 400\n",
    "N_HIDDEN_2 = 300\n",
    "\n",
    "class ActorNetwork(tf.Module):\n",
    "    \"\"\"Backbone Network for the Actor\"\"\"\n",
    "    \n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # Network parameters\n",
    "        self.W1 = tf.Variable(tf.random.uniform([num_states, N_HIDDEN_1], -1/math.sqrt(num_states), 1/math.sqrt(num_states)))\n",
    "        self.B1 = tf.Variable(tf.random.uniform([N_HIDDEN_1], -1/math.sqrt(num_states), 1/math.sqrt(num_states)))\n",
    "        self.W2 = tf.Variable(tf.random.uniform([N_HIDDEN_1, N_HIDDEN_2], -1/math.sqrt(N_HIDDEN_1), 1/math.sqrt(N_HIDDEN_1)))\n",
    "        self.B2 = tf.Variable(tf.random.uniform([N_HIDDEN_2], -1/math.sqrt(N_HIDDEN_1), 1/math.sqrt(N_HIDDEN_1)))\n",
    "        self.W3 = tf.Variable(tf.random.uniform([N_HIDDEN_2, num_actions], -0.003, 0.003))\n",
    "        self.B3 = tf.Variable(tf.random.uniform([num_actions], -0.003, 0.003))\n",
    "        \n",
    "        self.batch_norm1 = tf.keras.layers.BatchNormalization()\n",
    "        self.batch_norm2 = tf.keras.layers.BatchNormalization()\n",
    "    \n",
    "    def __call__(self, state, training):\n",
    "        h1_t = tf.matmul(state, self.W1)\n",
    "        h1_bn = self.batch_norm1(h1_t, training=training)\n",
    "        h1 = tf.nn.softplus(h1_bn) + self.B1\n",
    "        \n",
    "        h2_t = tf.matmul(h1, self.W2)\n",
    "        h2_bn = self.batch_norm2(h2_t, training=training)\n",
    "        h2 = tf.nn.tanh(h2_bn) + self.B2\n",
    "        \n",
    "        return tf.matmul(h2, self.W3) + self.B3\n",
    "    \n",
    "    def get_variables(self):\n",
    "        return [self.W1, self.B1, self.W2, self.B2, self.W3, self.B3]\n",
    "\n",
    "class Actor:\n",
    "    \"\"\"Actor that handles the network training and target update\"\"\"\n",
    "    \n",
    "    def __init__(self, num_states, num_actions):\n",
    "        # Initialize main and target networks\n",
    "        self.actor_network = ActorNetwork(num_states, num_actions)\n",
    "        self.target_network = ActorNetwork(num_states, num_actions)\n",
    "        \n",
    "        # Initialize target network with same weights as main network\n",
    "        self.update_target_actor(initial=True)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, epsilon=1e-08)\n",
    "\n",
    "    def evaluate_actor(self, state):\n",
    "        return self.actor_network(state, training=False)\n",
    "    \n",
    "    def evaluate_target_actor(self, state):\n",
    "        return self.target_network(state, training=False)\n",
    "    \n",
    "    def train_actor(self, actor_state_in, q_gradient_input):\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor_network(actor_state_in, training=True)\n",
    "            actor_parameters = self.actor_network.get_variables()\n",
    "            gradients = tape.gradient(actions, actor_parameters, output_gradients=-q_gradient_input)\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(gradients, actor_parameters))\n",
    "    \n",
    "    def update_target_actor(self, initial=False):\n",
    "        if initial:\n",
    "            for target_var, var in zip(self.target_network.get_variables(), self.actor_network.get_variables()):\n",
    "                target_var.assign(var)\n",
    "        else:\n",
    "            for target_var, var in zip(self.target_network.get_variables(), self.actor_network.get_variables()):\n",
    "                target_var.assign(TAU * var + (1 - TAU) * target_var)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329953f-cd5f-4ad1-915f-a028f4c92fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "TAU = 0.001\n",
    "BATCH_SIZE = 64\n",
    "N_HIDDEN_1 = 400\n",
    "N_HIDDEN_2 = 300\n",
    "\n",
    "class CriticNetwork(tf.Module):\n",
    "    \"\"\"Critic Q value model backbone with batch normalization for DDPG\"\"\"\n",
    "    \n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # Critic Network weights and biases\n",
    "        self.W1_c = tf.Variable(tf.random.uniform([num_states, N_HIDDEN_1], -1/math.sqrt(num_states), 1/math.sqrt(num_states)))\n",
    "        self.B1_c = tf.Variable(tf.random.uniform([N_HIDDEN_1], -1/math.sqrt(num_states), 1/math.sqrt(num_states)))\n",
    "        \n",
    "        self.W2_c = tf.Variable(tf.random.uniform([N_HIDDEN_1, N_HIDDEN_2], -1/math.sqrt(N_HIDDEN_1 + num_actions), 1/math.sqrt(N_HIDDEN_1 + num_actions)))\n",
    "        self.B2_c = tf.Variable(tf.random.uniform([N_HIDDEN_2], -1/math.sqrt(N_HIDDEN_1 + num_actions), 1/math.sqrt(N_HIDDEN_1 + num_actions)))\n",
    "        \n",
    "        self.W2_action_c = tf.Variable(tf.random.uniform([num_actions, N_HIDDEN_2], -1/math.sqrt(N_HIDDEN_1 + num_actions), 1/math.sqrt(N_HIDDEN_1 + num_actions)))\n",
    "        \n",
    "        self.W3_c = tf.Variable(tf.random.uniform([N_HIDDEN_2, 1], -0.003, 0.003))\n",
    "        self.B3_c = tf.Variable(tf.random.uniform([1], -0.003, 0.003))\n",
    "        \n",
    "        self.batch_norm1 = tf.keras.layers.BatchNormalization()\n",
    "        self.batch_norm2 = tf.keras.layers.BatchNormalization()\n",
    "    \n",
    "    def __call__(self, state, action, training):\n",
    "        h1_t = tf.matmul(state, self.W1_c)\n",
    "        h1_c_bn = self.batch_norm1(h1_t, training=training)\n",
    "        h1_c = tf.nn.softplus(h1_c_bn) + self.B1_c\n",
    "        \n",
    "        h2_t = tf.matmul(h1_c, self.W2_c) + tf.matmul(action, self.W2_action_c)\n",
    "        h2_c_bn = self.batch_norm2(h2_t, training=training)\n",
    "        h2_c = tf.nn.tanh(h2_c_bn) + self.B2_c\n",
    "        \n",
    "        q_value = tf.matmul(h2_c, self.W3_c) + self.B3_c\n",
    "        return q_value\n",
    "    \n",
    "    def get_variables(self):\n",
    "        return [self.W1_c, self.B1_c, self.W2_c, self.B2_c, self.W2_action_c, self.W3_c, self.B3_c]\n",
    "\n",
    "\n",
    "class Critic:\n",
    "    \"\"\"Critic that handles network training, target update, and other tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, num_states, num_actions):\n",
    "        # Initialize main and target networks\n",
    "        self.critic_network = CriticNetwork(num_states, num_actions)\n",
    "        self.target_network = CriticNetwork(num_states, num_actions)\n",
    "        \n",
    "        # Initialize target network with same weights as main network\n",
    "        self.update_target_critic(initial=True)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    def train_critic(self, state_batch, action_batch, y_batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.critic_network(state_batch, action_batch, training=True)\n",
    "            l2_loss = 0.0001 * tf.reduce_sum(tf.square(self.critic_network.W2_c))\n",
    "            cost = tf.reduce_mean(tf.square(q_values - y_batch)) + l2_loss\n",
    "        \n",
    "        critic_variables = self.critic_network.get_variables()\n",
    "        gradients = tape.gradient(cost, critic_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, critic_variables))\n",
    "\n",
    "    def evaluate_target_critic(self, state_batch, action_batch):\n",
    "        return self.target_network(state_batch, action_batch, training=False)\n",
    "    \n",
    "    def compute_delQ_a(self, state_batch, action_batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(action_batch)\n",
    "            q_values = self.critic_network(state_batch, action_batch, training=False)\n",
    "        return tape.gradient(q_values, action_batch)\n",
    "    \n",
    "    def update_target_critic(self, initial=False):\n",
    "        if initial:\n",
    "            for target_var, var in zip(self.target_network.get_variables(), self.critic_network.get_variables()):\n",
    "                target_var.assign(var)\n",
    "        else:\n",
    "            for target_var, var in zip(self.target_network.get_variables(), self.critic_network.get_variables()):\n",
    "                target_var.assign(TAU * var + (1 - TAU) * target_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080486b7-8654-4fbc-bf6f-71ff01abfe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "### batch_normol\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "decay = 0.95\n",
    "TAU = 0.001\n",
    "\n",
    "class batch_norm:\n",
    "    def __init__(self,inputs,size,is_training,sess,parForTarget=None,bn_param=None):\n",
    "        \n",
    "        self.sess = sess        \n",
    "        self.scale = tf.Variable(tf.random_uniform([size],0.9,1.1))\n",
    "        self.beta = tf.Variable(tf.random_uniform([size],-0.01,0.01))\n",
    "        self.pop_mean = tf.Variable(tf.random_uniform([size],-0.01,0.01),trainable=False)\n",
    "        self.pop_var = tf.Variable(tf.random_uniform([size],0.9,1.1),trainable=False)        \n",
    "        self.batch_mean, self.batch_var = tf.nn.moments(inputs,[0])        \n",
    "        self.train_mean = tf.assign(self.pop_mean,self.pop_mean * decay + self.batch_mean * (1 - decay))  \n",
    "        self.train_var = tf.assign(self.pop_var,self.pop_var * decay + self.batch_var * (1 - decay))\n",
    "                \n",
    "        def training(): \n",
    "            return tf.nn.batch_normalization(inputs,\n",
    "                self.batch_mean, self.batch_var, self.beta, self.scale, 0.0000001 )\n",
    "    \n",
    "        def testing(): \n",
    "            return tf.nn.batch_normalization(inputs,\n",
    "            self.pop_mean, self.pop_var, self.beta, self.scale, 0.0000001)\n",
    "        \n",
    "        if parForTarget!=None:\n",
    "            self.parForTarget = parForTarget\n",
    "            self.updateScale = self.scale.assign(self.scale*(1-TAU)+self.parForTarget.scale*TAU)\n",
    "            self.updateBeta = self.beta.assign(self.beta*(1-TAU)+self.parForTarget.beta*TAU)\n",
    "            self.updateTarget = tf.group(self.updateScale, self.updateBeta)\n",
    "    \n",
    "        self.bnorm = tf.cond(is_training,training,testing)\n",
    "    \n",
    "    def update_Target(self):\n",
    "        self.sess.run(self.updateBeta)\n",
    "        self.sess.run(self.updateScale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b8fc8eb-11e3-401c-9829-7eae889988f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## embedding\n",
    "\n",
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "\n",
    "class VideoGenreEmbedding(tf.keras.Model):\n",
    "    def __init__(self, len_videos, len_genres, embedding_dim):\n",
    "        super(VideoGenreEmbedding, self).__init__()\n",
    "        self.m_g_input = tf.keras.layers.InputLayer(name='input_layer', input_shape=(2,))\n",
    "        # embedding\n",
    "        self.m_embedding = tf.keras.layers.Embedding(name='video_embedding', input_dim=len_videos, output_dim=embedding_dim)\n",
    "        self.g_embedding = tf.keras.layers.Embedding(name='genre_embedding', input_dim=len_genres, output_dim=embedding_dim)\n",
    "        # dot product\n",
    "        self.m_g_merge = tf.keras.layers.Dot(name='video_genre_dot', normalize=True, axes=1)\n",
    "        # output\n",
    "        self.m_g_fc = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # x = self.m_g_input(x)\n",
    "        memb = self.m_embedding(x[0])\n",
    "        gemb = self.g_embedding(x[1])\n",
    "        m_g = self.m_g_merge([memb, gemb])\n",
    "        return self.m_g_fc(m_g)\n",
    "\n",
    "class UserVideoEmbedding(tf.keras.Model):\n",
    "    def __init__(self, len_users, len_videos, embedding_dim):\n",
    "        super(UserVideoEmbedding, self).__init__()\n",
    "        self.m_u_input = tf.keras.layers.InputLayer(name='input_layer', input_shape=(2,))\n",
    "        # embedding\n",
    "        self.u_embedding = tf.keras.layers.Embedding(name='user_embedding', input_dim=len_users, output_dim=embedding_dim)\n",
    "        self.m_embedding = tf.keras.layers.Embedding(name='video_embedding', input_dim=len_videos, output_dim=embedding_dim)\n",
    "        # dot product\n",
    "        self.m_u_merge = tf.keras.layers.Dot(name='video_user_dot', normalize=False, axes=1)\n",
    "        # output\n",
    "        self.m_u_fc = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, x):\n",
    "        # x = self.m_u_input(x)\n",
    "        uemb = self.u_embedding(x[0])\n",
    "        memb = self.m_embedding(x[1])\n",
    "        m_u = self.m_u_merge([memb, uemb])\n",
    "        return self.m_u_fc(m_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db887e4-4b01-4047-ab7a-1be33d3edbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enviroment\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "class StimulateEnv(object):\n",
    "    \n",
    "    def __init__(self, user_id, newest_watched_video, users_dict, users_history_lens, state_size):\n",
    "        \n",
    "        self.user_id = user_id\n",
    "        self.state_size = state_size \n",
    "        self.newest_watched_video = newest_watched_video\n",
    "        self.users_dict = users_dict\n",
    "        self.users_history_lens = users_history_lens\n",
    "        \n",
    "        self.user_items = {data[0]:data[1] for data in self.users_dict[self.user_id]} #{'video_id': 'rated'}\n",
    "        self.items = [data[0] for data in self.users_dict[self.user_id]] #[:self.state_size]]\n",
    "        self.done = False \n",
    "        self.old_watched = set(self.items) \n",
    "        self.done_count = 3000\n",
    "        \n",
    "    def step(self, recommend_item):\n",
    "        \n",
    "        reward = -0.5 \n",
    "        correctly_recommended = [] \n",
    "        rewards = [] \n",
    "        \n",
    "        if self.newest_watched_video in recommend_item and recommend_item not in self.old_watched:\n",
    "            correctly_recommended.append(recommend_item)\n",
    "            rewards.append(1)\n",
    "        else:\n",
    "            rewards.append(-0.5)\n",
    "        \n",
    "        deque_old_watched = deque(self.old_watched) \n",
    "        deque_old_watched.append(self.newest_watched_video)\n",
    "        deque_old_watched.popleft()\n",
    "\n",
    "        if max(rewards) > 0: \n",
    "            self.items = self.items[len(correctly_recommended):] + correctly_recommended \n",
    "        \n",
    "        reward = rewards\n",
    "        \n",
    "        if len(self.old_watched) > self.done_count or len(self.old_watched) >= self.users_history_lens:\n",
    "            self.done = True\n",
    "        \n",
    "        return self.items, reward, self.done, self.old_watched\n",
    "    \n",
    "    \"\"\" \n",
    "    def reset(self):\n",
    "        self.user_id = np.random.choice(self.)\n",
    "        \n",
    "    Too lazy, this part will serves only stimulation stage when RESET auto generate random user\n",
    "    and other information to start the algo\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327bbeb8-3b51-4f69-a855-d1b106a3797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ou noise\n",
    "\n",
    "# --------------------------------------\n",
    "# Ornstein-Uhlenbeck Noise\n",
    "# Author: Flood Sung\n",
    "# Date: 2016.5.4\n",
    "# Reference: https://github.com/rllab/rllab/blob/master/rllab/exploration_strategies/ou_strategy.py\n",
    "# --------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"docstring for OUNoise\"\"\"\n",
    "    def __init__(self,action_dimension,mu=0, theta=0.15, sigma=0.2):\n",
    "        self.action_dimension = action_dimension\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "\n",
    "    def noise(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * nr.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ou = OUNoise(3)\n",
    "    states = []\n",
    "    for i in range(1000):\n",
    "        states.append(ou.noise())\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.plot(states)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70d17c-8dbe-4a5f-a26d-724ab9f8da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## replay \n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.num_experiences = 0\n",
    "        self.buffer = deque()\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # Randomly sample batch_size examples\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def size(self):\n",
    "        return self.buffer_size\n",
    "\n",
    "    def add(self, state, action, reward, new_state, done):\n",
    "        experience = (state, action, reward, new_state, done)\n",
    "        if self.num_experiences < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.num_experiences += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def count(self):\n",
    "        # if buffer is full, return buffer size\n",
    "        # otherwise, return experience counter\n",
    "        return self.num_experiences\n",
    "\n",
    "    def erase(self):\n",
    "        self.buffer = deque()\n",
    "        self.num_experiences = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc199c-87d2-4ff6-b05c-a76e7b66a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## state represent\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class DRRAveStateRepresentation(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, output_dim):\n",
    "        super(DRRAveStateRepresentation, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.wav = tf.keras.layers.Conv1D(1, 1, 1)\n",
    "        self.concat = tf.keras.layers.Concatenate()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.dense = tf.keras.layers.Dense(output_dim, activation=None)\n",
    "        \n",
    "    def call(self, x):\n",
    "        items_eb = tf.transpose(x[1], perm=(0,2,1))/self.embedding_dim\n",
    "        wav = self.wav(items_eb)\n",
    "        wav = tf.transpose(wav, perm=(0,2,1))\n",
    "        wav = tf.squeeze(wav, axis=1)\n",
    "        user_wav = tf.keras.layers.multiply([x[0], wav])\n",
    "        concat = self.concat([x[0], user_wav, wav])\n",
    "        \n",
    "        flattened = self.flatten(concat)\n",
    "        output = self.dense(flattened)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef790ff2-0c0c-4f50-9406-145b7d4fd1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tensor grad_inverted\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class GradInverter:\n",
    "    def __init__(self, action_bounds):\n",
    "        self.action_size = len(action_bounds[0])\n",
    "        self.pmax = tf.constant(action_bounds[0], dtype=tf.float32)\n",
    "        self.pmin = tf.constant(action_bounds[1], dtype=tf.float32)\n",
    "        self.prange = tf.constant([x - y for x, y in zip(action_bounds[0], action_bounds[1])], dtype=tf.float32)\n",
    "\n",
    "    def invert(self, grad, action):\n",
    "        # Ensure inputs are tensors\n",
    "        action_input = tf.convert_to_tensor(action, dtype=tf.float32)\n",
    "        act_grad = tf.convert_to_tensor(grad, dtype=tf.float32)\n",
    "\n",
    "        pdiff_max = (-action_input + self.pmax) / self.prange\n",
    "        pdiff_min = (action_input - self.pmin) / self.prange\n",
    "        zeros_act_grad_filter = tf.zeros_like(act_grad)\n",
    "\n",
    "        # Perform element-wise comparison and selection\n",
    "        grad_inverter = tf.where(tf.greater(act_grad, zeros_act_grad_filter), act_grad * pdiff_max, act_grad * pdiff_min)\n",
    "        \n",
    "        return grad_inverter.numpy()  # Convert the result back to numpy if necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df48f2e4-0cb8-450a-b49b-d5c84b58b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "https://github.com/cookbenjamin/DDPG/blob/master/ddpg.py : Lấy thông tin note các files \n",
    "\n",
    "\n",
    "### Tham khảo code\n",
    "\n",
    "https://github.com/openai/baselines/tree/master/baselines/ddpg: OpenAI \n",
    "https://github.com/stevenpjg/ddpg-aigym/tree/master: CHÍNH \n",
    "\n",
    "https://yanpanlau.github.io/2016/10/11/Torcs-Keras.html: tham khảo\n",
    "https://github.com/yanpanlau/DDPG-Keras-Torcs/blob/master/ddpg.py : kham thảo \n",
    "\n",
    "https://github.com/m5823779/motion-planner-reinforcement-learning\n",
    "https://github.com/m5823779/DDPG/blob/master/ddpg/action_dim%3D1%20(success)/ddpg.py : cấu trúc file rõ ràng - CHÍNH\n",
    "\n",
    "https://github.com/samkoesnadi/DDPG-tf2/blob/master/src/model.py: chưa thấy phần Batch-normalization\n",
    "\n",
    "### Đọc thêm \n",
    "https://github.com/ghliu/pytorch-ddpg?tab=readme-ov-file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bffe4bb-6804-406f-a70b-7acc4c4850ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ddpg \n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from model.embedding import UserVideoEmbedding\n",
    "import sys,os\n",
    "sys.path.append(os.getcwd()) \n",
    "\n",
    "from model.actor import Actor\n",
    "from model.critic import Critic\n",
    "from model.tensorflow_grad_inverter import GradInverter as grad_inverter\n",
    "from model.state_representation import DRRAveStateRepresentation\n",
    "\n",
    "REPLAY_MEMORY_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA=0.99\n",
    "is_grad_inverter = True\n",
    "\n",
    "cwd = '/home/tuannm84/Desktop/myclip/vtcc-myclip-recommender-system-v2/myclip_recommender_v2/asset/'\n",
    "class DDPG(object):\n",
    "    \"\"\" Deep Deterministic Policy Gradient Algorithm \"\"\"\n",
    "\n",
    "    def __init__(self, env, users_num, items_num, num_actions, STATE_SIZE, output_dim):\n",
    "        self.env = env\n",
    "        self.num_states = STATE_SIZE\n",
    "        self.num_actions = num_actions ## Number of video to be choosed\n",
    "\n",
    "        # Initialize Actor and Critic networks\n",
    "        self.critic_net = Critic(self.num_states, self.num_actions)\n",
    "        self.actor_net = Actor(self.num_states, self.num_actions)\n",
    "\n",
    "        # Initialize Replay Memory\n",
    "        self.replay_memory = deque()\n",
    "\n",
    "        # Initialize time step\n",
    "        self.time_step = 0\n",
    "        self.counter = 0\n",
    "\n",
    "        action_max = [num_actions]\n",
    "        action_min = [1]\n",
    "        action_bounds = [action_max, action_min]\n",
    "        self.grad_inv = grad_inverter(action_bounds)\n",
    "        \n",
    "        self.embedding_dim = 100\n",
    "        self.embedding_network = UserVideoEmbedding(users_num, items_num, self.embedding_dim)\n",
    "        embedding_save_file_dir = os.path.join(cwd, 'dataset/save_weights/user_movie_embedding_case4.h5') #m_g_model_weights.weights.h5'\n",
    "        assert os.path.exists(embedding_save_file_dir), f\"embedding save file directory: '{embedding_save_file_dir}' is wrong.\"\n",
    "        self.embedding_network.built = True\n",
    "        self.embedding_network.load_weights(embedding_save_file_dir, by_name = True, skip_mismatch = True)\n",
    "        \n",
    "        self.srm_ave = DRRAveStateRepresentation(self.embedding_dim, output_dim)\n",
    "        self.srm_ave([np.zeros((1, 100,)),np.zeros((1, STATE_SIZE, 100))])\n",
    "        \n",
    "    def evaluate_actor(self, state_t):\n",
    "        return self.actor_net.evaluate_actor(state_t)\n",
    "\n",
    "    def add_experience(self, observation_1, observation_2, action, reward, done):\n",
    "        self.observation_1 = observation_1 # previous \n",
    "        self.observation_2 = observation_2 # newest \n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.done = done\n",
    "        self.replay_memory.append((self.observation_1, self.observation_2, self.action, self.reward, self.done))\n",
    "        self.time_step += 1\n",
    "        if len(self.replay_memory) > REPLAY_MEMORY_SIZE:\n",
    "            self.replay_memory.popleft()\n",
    "\n",
    "    def minibatches(self):\n",
    "        batch = random.sample(self.replay_memory, BATCH_SIZE)\n",
    "        # state t\n",
    "        self.state_t_batch = np.array([item[0] for item in batch])\n",
    "        # state t+1\n",
    "        self.state_t_1_batch = np.array([item[1] for item in batch])\n",
    "        self.action_batch = np.array([item[2] for item in batch]).reshape(len(batch), self.num_actions)\n",
    "        self.reward_batch = np.array([item[3] for item in batch])\n",
    "        self.done_batch = np.array([item[4] for item in batch])\n",
    "\n",
    "    def train(self):\n",
    "        # Sample a random minibatch of N transitions from replay memory\n",
    "        self.minibatches()\n",
    "        self.action_t_1_batch = self.actor_net.evaluate_target_actor(self.state_t_1_batch)\n",
    "        # Q'(s_i+1,a_i+1)\n",
    "        q_t_1 = self.critic_net.evaluate_target_critic(self.state_t_1_batch, self.action_t_1_batch)\n",
    "        self.y_i_batch = []\n",
    "\n",
    "        for i in range(BATCH_SIZE):\n",
    "            if self.done_batch[i]:\n",
    "                self.y_i_batch.append(self.reward_batch[i])\n",
    "            else:\n",
    "                self.y_i_batch.append(self.reward_batch[i] + GAMMA * q_t_1[i][0])\n",
    "\n",
    "        self.y_i_batch = np.array(self.y_i_batch).reshape(len(self.y_i_batch), 1)\n",
    "\n",
    "        # Update critic by minimizing the loss\n",
    "        self.critic_net.train_critic(self.state_t_batch, self.action_batch, self.y_i_batch)\n",
    "\n",
    "        # Update actor proportional to the gradients:\n",
    "        action_for_delQ = self.evaluate_actor(self.state_t_batch)\n",
    "\n",
    "        if is_grad_inverter:\n",
    "            self.del_Q_a = self.critic_net.compute_delQ_a(self.state_t_batch, action_for_delQ)\n",
    "            self.del_Q_a = self.grad_inv.invert(self.del_Q_a, action_for_delQ)\n",
    "        else:\n",
    "            self.del_Q_a = self.critic_net.compute_delQ_a(self.state_t_batch, action_for_delQ)[0]\n",
    "\n",
    "        # Train actor network proportional to delQ/dela and del_Actor_model/del_actor_parameters:\n",
    "        self.actor_net.train_actor(self.state_t_batch, self.del_Q_a)\n",
    "\n",
    "        # Update target Critic and Actor networks\n",
    "        self.critic_net.update_target_critic()\n",
    "        self.actor_net.update_target_actor()\n",
    "                     \n",
    "                     \n",
    "    def recommend_item(self, action, all_items, old_watched, top_k=False, items_ids=None):\n",
    "        if items_ids is None:\n",
    "            items_ids = np.array(list(set(all_items) - set(old_watched)))\n",
    "            # items_ids = np.array(list(set(self.items_list) - set(recommended_items)))\n",
    "\n",
    "        \n",
    "        items_ebs = self.embedding_network.get_layer('video_embedding')(items_ids)\n",
    "        action = tf.expand_dims(action, axis=1)\n",
    "        if top_k:\n",
    "            item_indice = np.argsort(tf.transpose(tf.reduce_sum((items_ebs* action), axis=1, keepdims=True), perm=(1, 0)))[0][-top_k:]\n",
    "            return items_ids[item_indice]\n",
    "        else:\n",
    "            item_idx = np.argmax(tf.transpose(tf.reduce_sum((items_ebs* action), axis=1, keepdims=True), perm=(1, 0)))\n",
    "            return items_ids[item_idx]\n",
    "                 \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5410c-5270-435f-9123-ff4ef8286188",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.ops.gen_math_ops import Exp\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import sys \n",
    "# sys.path.append('/home/tuannm84/Desktop/longbien/Project/MC/')\n",
    "\n",
    "from model.actor import Actor\n",
    "from model.critic import Critic\n",
    "from model.enviroment import StimulateEnv\n",
    "from model.ddpg import DDPG \n",
    "from model.embedding import VideoGenreEmbedding, UserVideoEmbedding\n",
    "from model.ou_noise import OUNoise\n",
    "\n",
    "\n",
    "Path = '/home/tuannm84/Desktop/longbien/Project/MC'\n",
    "\n",
    "PATH_USER_DICT = os.path.join(Path, \"dataset/user_dict.npy\")\n",
    "PATH_TRAIN_DATASET = os.path.join(Path, \"dataset/train_dict.npy\")\n",
    "PATH_EVAL_DATSET = os.path.join(Path, \"dataset/eval_dict.npy\")\n",
    "PATH_USER_HISTORY_LENS = os.path.join(Path, 'dataset/users_history_len_local.npy')\n",
    "PATH_DICTIONARY = os.path.join(Path, \"dataset/dictionary.npy\")\n",
    "PATH_DATA_NUMBER = os.path.join(Path, \"dataset/data_number.npy\")\n",
    "\n",
    "users_dict = np.load(PATH_USER_DICT,allow_pickle='TRUE').item()\n",
    "eval_users_dict = np.load(PATH_EVAL_DATSET,allow_pickle='TRUE').item()\n",
    "train_users_dict = np.load(PATH_TRAIN_DATASET,allow_pickle='TRUE').item()\n",
    "dictionary = np.load(PATH_DICTIONARY,allow_pickle='TRUE').item()\n",
    "data_number = np.load(PATH_DATA_NUMBER,allow_pickle='TRUE').item()\n",
    "users_history_lens = np.load(PATH_USER_HISTORY_LENS, allow_pickle='TRUE')\n",
    "all_items = {data[0] for i, k in users_dict.items() for data in k}  ## list video toan tap data \n",
    "\n",
    "user_dataset = eval_users_dict\n",
    "user_id = 10 \n",
    "users_history_lens = len(user_dataset[user_id])\n",
    "newest_watched_video = np.random.choice([i[0] for i in user_dataset[11]])\n",
    "watched_videos = [data[0] for data in users_dict[user_id]]\n",
    "items_ids = np.array(list(set(all_items) - set(watched_videos)))\n",
    "len_items_ids = len(items_ids)\n",
    "STATE_SIZE = len_items_ids ## 1445 là số lượng videos sau khi trừ đi các video đã xem trong history\n",
    "num_actions = len_items_ids\n",
    "output_dim = len_items_ids\n",
    "users_num = data_number['users_num']\n",
    "items_num = data_number['items_num']\n",
    "\n",
    "env_prod = StimulateEnv(user_id, newest_watched_video, users_dict, users_history_lens, STATE_SIZE)\n",
    "recommender  = DDPG(env_prod, users_num, items_num, num_actions, STATE_SIZE, output_dim)  # output_dim là output của State_emebedding, để 1445 vì đầu vào của actor.evaluate_actor là (1445,400)\n",
    "\n",
    "EMBEDDING_SIZE = 100\n",
    "epsilon_for_priority = 1e-6\n",
    "batch_size = 32\n",
    "num_actions = len_items_ids ## Number of list video to be choosed\n",
    "num_actions = len_items_ids\n",
    "exploration_noise = OUNoise(num_actions)\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "\"\"\"\n",
    "CASE 1: STATE from ITEMS_IDS => feed into EVALUATE_ACTOR => ACTION  \n",
    "\"\"\"\n",
    "x = items_ids\n",
    "array_x = np.reshape(x,[1, num_actions])\n",
    "state_value = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "state_value = tf.expand_dims(state_value, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "CASE 2: STATE from ITEM_IDS + STATE_REPRESENTATION + USER_EMDS => STATE => feed into EVALUATE_ACTOR => ACTION \n",
    "\"\"\"\n",
    "user_eb = recommender.embedding_network.get_layer('user_embedding')(np.array(user_id))\n",
    "items_eb = recommender.embedding_network.get_layer('video_embedding')(np.array(items_ids))\n",
    "state = recommender.srm_ave([np.expand_dims(user_eb, axis=0), np.expand_dims(items_eb, axis=0)])\n",
    "action = recommender.evaluate_actor(state)\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "action = recommender.evaluate_actor(state_value)\n",
    "noise = exploration_noise.noise()\n",
    "action = action[0] + noise\n",
    "recommended_item = recommender.recommend_item(action, all_items, env_prod.old_watched, top_k= 5)\n",
    "next_items_ids_embs, reward, done, _ = env_prod.step(recommended_item)\n",
    "reward = np.sum(reward)\n",
    "\n",
    "# agent.add_experience(state_value, next_items_ids,action,reward,done)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "2 cách trên đang ra cùng 1 kết quả (xem tại jupyter notebook)\n",
    "\n",
    "tiếp theo: Hoàn thiện Circle workflow - phần train và phần minibatch thử chạy xem tiếp ntn và cái replay hoạt động ổn k ra sao\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3944fbaf-f366-40a9-8674-c4c52c93e12c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
