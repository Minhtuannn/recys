{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.ops.gen_math_ops import Exp\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import sys \n",
    "\n",
    "from model.actor import Actor\n",
    "from model.critic import Critic\n",
    "from model.enviroment import StimulateEnv\n",
    "from model.ddpg import DDPG \n",
    "from model.embedding import VideoGenreEmbedding, UserVideoEmbedding\n",
    "from model.ou_noise import OUNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub = '/Users/minhtuan/Documents/Documents/Work/Hanoi/Hub'\n",
    "path = '/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/X/movielens/airflow_folder/drl_melody/storage/dataset1M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "StimulateEnv.__init__() takes 3 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m num_actions \u001b[38;5;241m=\u001b[39m  \u001b[38;5;241m5\u001b[39m \u001b[38;5;66;03m#len_items_ids ## Number of list video to be choosed\u001b[39;00m\n\u001b[1;32m     27\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;66;03m#len_items_ids\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m env_prod \u001b[38;5;241m=\u001b[39m \u001b[43mStimulateEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewest_watched_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_dict_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musers_history_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTATE_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m recommender  \u001b[38;5;241m=\u001b[39m DDPG(env_prod, users_num, items_num, num_actions, STATE_SIZE, output_dim)  \u001b[38;5;66;03m# output_dim là output của State_emebedding, để 1445 vì đầu vào của actor.evaluate_actor là (1445,400)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m EMBEDDING_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: StimulateEnv.__init__() takes 3 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "PATH_USER_DICT = os.path.join(hub, \"user_dict.npy\")\n",
    "PATH_EVAL_DATSET = os.path.join(hub, \"eval_dict.npy\")\n",
    "PATH_USER_HISTORY_LENS = os.path.join(hub, 'users_history_len.npy')\n",
    "PATH_DATA_NUMBER = os.path.join(hub, \"data_number.npy\")\n",
    "\n",
    "users_dict = np.load(PATH_USER_DICT,allow_pickle='TRUE').item()\n",
    "eval_users_dict = np.load(PATH_EVAL_DATSET,allow_pickle='TRUE').item()\n",
    "data_number = np.load(PATH_DATA_NUMBER,allow_pickle='TRUE').item()\n",
    "users_history_lens = np.load(PATH_USER_HISTORY_LENS, allow_pickle='TRUE')\n",
    "all_items = {data[0] for i, k in users_dict.items() for data in k}  ## list video toan tap data \n",
    "\n",
    "user_dataset = eval_users_dict\n",
    "user_id = 4834\n",
    "\n",
    "users_history_lens = round(len(user_dataset[user_id]) * 0.6) ## split data in EVAL into 2 pieces: one for history and other for streaming \n",
    "watched_videos = [data[0] for data in user_dataset[user_id]][:users_history_lens]\n",
    "user_dict_history = {user_id: eval_users_dict[user_id][:users_history_lens]}\n",
    "# newest_watched_video = np.random.choice([i[0] for i in user_dataset[user_id]])\n",
    "newest_watched_video = [data[0] for data in user_dataset[user_id]][users_history_lens:][0]\n",
    "items_ids = np.array(list(set(all_items) - set(watched_videos)))\n",
    "len_items_ids = len(items_ids)\n",
    "users_num = data_number['users_num']\n",
    "items_num = data_number['items_num']\n",
    "\n",
    "STATE_SIZE =  5 #len_items_ids ## 1445 là số lượng videos sau khi trừ đi các video đã xem trong history\n",
    "num_actions =  5 #len_items_ids ## Number of list video to be choosed\n",
    "output_dim = 5 #len_items_ids\n",
    "\n",
    "env_prod = StimulateEnv(user_id, newest_watched_video, user_dict_history, users_history_lens, STATE_SIZE)\n",
    "recommender  = DDPG(env_prod, users_num, items_num, num_actions, STATE_SIZE, output_dim)  # output_dim là output của State_emebedding, để 1445 vì đầu vào của actor.evaluate_actor là (1445,400)\n",
    "\n",
    "\n",
    "EMBEDDING_SIZE = 100\n",
    "epsilon_for_priority = 1e-6\n",
    "batch_size = 32\n",
    "exploration_noise = OUNoise(num_actions)\n",
    "\n",
    "## item_ids should comes from history of user \n",
    "\n",
    "# x = items_ids\n",
    "x = watched_videos[- STATE_SIZE:]\n",
    "array_x = np.reshape(x,[1, num_actions])\n",
    "state_value = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "state_value = tf.expand_dims(state_value, axis=0)\n",
    "\n",
    "action = recommender.evaluate_actor(state_value) ## create a ranking video weight \n",
    "noise = exploration_noise.noise()\n",
    "action = action[0] + noise\n",
    "\n",
    "recommended_item = recommender.recommend_item(action, all_items, env_prod.old_watched, top_k= 5) ## create a list of suggested video \n",
    "next_items_ids_embs, reward, done, _ = env_prod.step(recommended_item)\n",
    "\n",
    "array_x_next = np.reshape(next_items_ids_embs,[1, num_actions])\n",
    "state_value_next = tf.convert_to_tensor(next_items_ids_embs, dtype=tf.float32)\n",
    "state_value_next = tf.expand_dims(state_value_next, axis=0)\n",
    "\n",
    "reward = np.sum(reward)\n",
    "\n",
    "recommender.add_experience(state_value, state_value_next, action, reward, done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note flow \n",
    "\n",
    "reset will take out 5 videos from history of user, 5 videos \n",
    "        -> embedding -> state -> actor.network \n",
    "        -> action (ranking video base on user) -> feed on recommender.recommend_item (take out video to suggest) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2538,  851,  838, 1880, 2900]),\n",
       " [2395, 2762, 2724, 314, 2702],\n",
       " 2840,\n",
       " [2395, 2762, 2724, 314, 2702])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_item, env_prod.old_watched, newest_watched_video, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/9: finish flow evaluate_actor and predict\n",
    "\n",
    "next step: replay_buffer and train sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_USER_DICT = os.path.join(hub, \"user_dict.npy\")\n",
    "PATH_EVAL_DATSET = os.path.join(hub, \"eval_dict.npy\")\n",
    "PATH_USER_HISTORY_LENS = os.path.join(hub, 'users_history_len.npy')\n",
    "PATH_DATA_NUMBER = os.path.join(hub, \"data_number.npy\")\n",
    "\n",
    "users_dict = np.load(PATH_USER_DICT,allow_pickle='TRUE').item()\n",
    "eval_users_dict = np.load(PATH_EVAL_DATSET,allow_pickle='TRUE').item()\n",
    "data_number = np.load(PATH_DATA_NUMBER,allow_pickle='TRUE').item()\n",
    "users_history_lens = np.load(PATH_USER_HISTORY_LENS, allow_pickle='TRUE')\n",
    "all_items = {data[0] for i, k in users_dict.items() for data in k}  ## list video toan tap data \n",
    "\n",
    "user_dataset = eval_users_dict\n",
    "users_num = data_number['users_num']\n",
    "items_num = data_number['items_num']\n",
    "\n",
    "STATE_SIZE = 5\n",
    "num_actions = 5\n",
    "output_dim = 5\n",
    "\n",
    "user_id = 4833\n",
    "users_history_lens = round(len(eval_users_dict[user_id]) * 0.6)\n",
    "watched_videos = [video[0] for video in eval_users_dict[user_id]][:users_history_lens]\n",
    "newest_watched_video = [video[0] for video in eval_users_dict[user_id]][users_history_lens:][0]\n",
    "enviroment = StimulateEnv(user_id, newest_watched_video, eval_users_dict, users_history_lens,STATE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.embedding.Embedding object at 0x15e5bd1e0>. Skipping object. Exception encountered: Layer 'user_embedding' expected 0 variables, but received 1 variables during loading. Expected: []\n",
      "  _load_state(\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.embedding.Embedding object at 0x15e5bcee0>. Skipping object. Exception encountered: Layer 'video_embedding' expected 0 variables, but received 1 variables during loading. Expected: []\n",
      "  _load_state(\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.dense.Dense object at 0x15e5beaa0>. Skipping object. Exception encountered: Layer 'dense_6' expected 0 variables, but received 2 variables during loading. Expected: []\n",
      "  _load_state(\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.embedding.Embedding object at 0x15b774eb0>. Skipping object. Exception encountered: Layer 'user_embedding' expected 0 variables, but received 1 variables during loading. Expected: []\n",
      "  _load_state(\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.embedding.Embedding object at 0x15b777b50>. Skipping object. Exception encountered: Layer 'video_embedding' expected 0 variables, but received 1 variables during loading. Expected: []\n",
      "  _load_state(\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.dense.Dense object at 0x15e33a440>. Skipping object. Exception encountered: Layer 'dense_8' expected 0 variables, but received 2 variables during loading. Expected: []\n",
      "  _load_state(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Starting episode no: 0 ==== \n",
      "\n",
      "[[ 750.  608.  919. 1198. 1196.]]\n",
      "3504 tf.Tensor([0.02020714 0.24790782 0.02709563 0.41056746 0.297307  ], shape=(5,), dtype=float32)\n",
      "________________________________________________________________\n",
      "[[1259. 1208.  508. 1265. 3361.]]\n",
      "[[ 608.  919. 1198. 1196. 3504.]]\n",
      "3504 tf.Tensor([ 0.327122    0.03632209 -0.20857538  0.09027514  0.3780679 ], shape=(5,), dtype=float32)\n",
      "________________________________________________________________\n",
      "[[1259. 1208.  508. 1265. 3361.]]\n",
      "[[ 919. 1198. 1196. 3504. 1234.]]\n",
      "3504 tf.Tensor([ 0.95536685 -0.19035113 -0.39501965  0.21053302  0.5716473 ], shape=(5,), dtype=float32)\n",
      "________________________________________________________________\n",
      "[[1259. 1208.  508. 1265. 3361.]]\n",
      "[[1198. 1196. 3504. 1234.  593.]]\n",
      "3504 tf.Tensor([ 0.8368488  -0.08864509 -0.07643326  0.14995944  0.22336231], shape=(5,), dtype=float32)\n",
      "________________________________________________________________\n",
      "[[1259. 1208.  508. 1265. 3361.]]\n",
      "[[1196. 3504. 1234.  593. 1304.]]\n",
      "3504 tf.Tensor([ 0.7330743   0.05314501  0.08961719  0.16348308 -0.12995982], shape=(5,), dtype=float32)\n",
      "________________________________________________________________\n",
      "[[1259. 1208.  508. 1265. 3361.]]\n",
      "[[3504. 1234.  593. 1304. 1247.]]\n",
      "3504 tf.Tensor([ 0.76656115 -0.18165562 -0.14546666  0.19717962  0.12640974], shape=(5,), dtype=float32)\n",
      "________________________________________________________________\n",
      "[[1259. 1208.  508. 1265. 3361.]]\n",
      "[[1234.  593. 1304. 1247.   34.]]\n",
      "3504 tf.Tensor([ 0.6607094  -0.16062987 -0.05994144  0.10076928  0.0547392 ], shape=(5,), dtype=float32)\n",
      "________________________________________________________________\n",
      "[[1259. 1208.  508. 1265. 3361.]]\n",
      "[[[3504. 1234.  593. 1304. 1247.]]\n",
      "\n",
      " [[ 608.  919. 1198. 1196. 3504.]]\n",
      "\n",
      " [[1196. 3504. 1234.  593. 1304.]]\n",
      "\n",
      " [[ 750.  608.  919. 1198. 1196.]]\n",
      "\n",
      " [[1198. 1196. 3504. 1234.  593.]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"batch_normalization_36\" is incompatible with the layer: expected ndim=2, found ndim=3. Full shape received: (5, 1, 400)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#train critic and actor network\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m counter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5\u001b[39m: \n\u001b[0;32m---> 51\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m reward_per_episode\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mreward[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     53\u001b[0m counter\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Documents/Work/Hanoi/DDPG/model/ddpg.py:104\u001b[0m, in \u001b[0;36mDDPG.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Update actor proportional to the gradients:\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_t_batch)\n\u001b[0;32m--> 104\u001b[0m action_for_delQ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_t_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_grad_inverter:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdel_Q_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_net\u001b[38;5;241m.\u001b[39mcompute_delQ_a(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_t_batch, action_for_delQ)\n",
      "File \u001b[0;32m~/Documents/Documents/Work/Hanoi/DDPG/model/ddpg.py:60\u001b[0m, in \u001b[0;36mDDPG.evaluate_actor\u001b[0;34m(self, state_t)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_actor\u001b[39m(\u001b[38;5;28mself\u001b[39m, state_t):\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_t\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Documents/Work/Hanoi/DDPG/model/actor.py:57\u001b[0m, in \u001b[0;36mActor.evaluate_actor\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_actor\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Documents/Work/Hanoi/DDPG/model/actor.py:30\u001b[0m, in \u001b[0;36mActorNetwork.__call__\u001b[0;34m(self, state, training)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, training):\n\u001b[1;32m     29\u001b[0m     h1_t \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmatmul(state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW1)\n\u001b[0;32m---> 30\u001b[0m     h1_bn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh1_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     h1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftplus(h1_bn) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB1\n\u001b[1;32m     33\u001b[0m     h2_t \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmatmul(h1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW2)\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/engine/input_spec.py:235\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m shape\u001b[38;5;241m.\u001b[39mrank\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m--> 235\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    237\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    239\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m         )\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"batch_normalization_36\" is incompatible with the layer: expected ndim=2, found ndim=3. Full shape received: (5, 1, 400)"
     ]
    }
   ],
   "source": [
    "\n",
    "#specify parameters here:\n",
    "episodes=10000\n",
    "\n",
    "#Randomly initialize critic,actor,target critic, target actor network  and replay buffer   \n",
    "agent  = DDPG(enviroment, users_num, items_num, num_actions, STATE_SIZE, output_dim)  # output_dim là output của State_emebedding, để 1445 vì đầu vào của actor.evaluate_actor là (1445,400)\n",
    "exploration_noise = OUNoise(num_actions)\n",
    "counter=0\n",
    "reward_per_episode = 0    \n",
    "total_reward=0\n",
    "steps = 20\n",
    "#saving reward:\n",
    "reward_st = np.array([0])\n",
    "    \n",
    "\n",
    "for i in range(0, episodes):\n",
    "    print(\"==== Starting episode no:\",i,\"====\",\"\\n\")\n",
    "    user_id, watched_videos, done = enviroment.reset()\n",
    "\n",
    "    # user_id = 4833\n",
    "    users_history_lens = round(len(eval_users_dict[user_id]) * 0.6)\n",
    "    watched_videos = [video[0] for video in eval_users_dict[user_id]][:users_history_lens]\n",
    "    newest_watched_video = [video[0] for video in eval_users_dict[user_id]][users_history_lens:]\n",
    "    newest_watched_video_start = [video[0] for video in eval_users_dict[user_id]][users_history_lens:][i]\n",
    "    enviroment = StimulateEnv(user_id, newest_watched_video_start, eval_users_dict, users_history_lens,STATE_SIZE)\n",
    "    agent  = DDPG(enviroment, users_num, items_num, num_actions, STATE_SIZE, output_dim)  # output_dim là output của State_emebedding, để 1445 vì đầu vào của actor.evaluate_actor là (1445,400)\n",
    "\n",
    "    # users_history_lens =  round(len(user_dataset[user_id]) * 0.6)\n",
    "    # watched_videos =  [data[0] for data in eval_users_dict[user_id]][:users_history_lens]\n",
    "    reward_per_episode = 0\n",
    "    for t in range(0, steps):\n",
    "        x = watched_videos[- STATE_SIZE:]\n",
    "        ## change shape to fit evaluate_actor \n",
    "        state_value = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "        state_value = tf.expand_dims(state_value, axis=0)\n",
    "        print(np.reshape(state_value,[1, num_actions]))\n",
    "        action = agent.evaluate_actor(np.reshape(state_value,[1, num_actions]))\n",
    "        noise = exploration_noise.noise()\n",
    "        action = action[0] + noise #Select action according to current policy and exploration noise\n",
    "        # print(\"Action at step\", t ,\" :\",action,\"\\n\")\n",
    "        \n",
    "        next_items_ids_embs, reward, done, _= enviroment.step(action)\n",
    "        state_value_next = tf.convert_to_tensor(next_items_ids_embs, dtype=tf.float32)\n",
    "        state_value_next = tf.expand_dims(state_value_next, axis=0)\n",
    "        state_value_next = np.reshape(state_value_next,[1, num_actions])\n",
    "        print(\"_\"*64)\n",
    "        print(state_value_next)\n",
    "        #add s_t,s_t+1,action,reward to experience memory\n",
    "        agent.add_experience(state_value, state_value_next, action, reward, done)\n",
    "        #train critic and actor network\n",
    "        if counter > 5: \n",
    "            agent.train()\n",
    "        reward_per_episode+=reward[0]\n",
    "        counter+=1\n",
    "        #check if episode ends:\n",
    "        if (done or (t == steps-1)):\n",
    "            print('EPISODE: ',i,' Steps: ',t,' Total Reward: ',reward_per_episode)\n",
    "            print(\"Printing reward to file\")\n",
    "            exploration_noise.reset() #reinitializing random noise for action exploration\n",
    "            reward_st = np.append(reward_st,reward_per_episode)\n",
    "            np.savetxt('episode_reward.txt',reward_st, newline=\"\\n\")\n",
    "            print('\\n\\n')\n",
    "            break\n",
    "        watched_videos.append(newest_watched_video[t])\n",
    "total_reward+=reward_per_episode            \n",
    "print(\"Average reward per episode {}\".format(total_reward / episodes)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(random.sample(agent.replay_memory, 5))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hanoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
