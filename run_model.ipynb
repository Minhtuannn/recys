{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.ops.gen_math_ops import Exp\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import sys \n",
    "\n",
    "from model.actor import Actor\n",
    "from model.critic import Critic\n",
    "from model.enviroment import StimulateEnv\n",
    "from model.ddpg import DDPG \n",
    "from model.embedding import VideoGenreEmbedding, UserVideoEmbedding\n",
    "from model.ou_noise import OUNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub = '/Users/minhtuan/Documents/Documents/Work/Hanoi/Hub'\n",
    "path = '/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/X/movielens/airflow_folder/drl_melody/storage/dataset1M'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.embedding.Embedding object at 0x17755e320>. Skipping object. Exception encountered: Layer 'user_embedding' expected 0 variables, but received 1 variables during loading. Expected: []\n",
      "  _load_state(\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.embedding.Embedding object at 0x17755df60>. Skipping object. Exception encountered: Layer 'video_embedding' expected 0 variables, but received 1 variables during loading. Expected: []\n",
      "  _load_state(\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.dense.Dense object at 0x17755d990>. Skipping object. Exception encountered: Layer 'dense_4' expected 0 variables, but received 2 variables during loading. Expected: []\n",
      "  _load_state(\n"
     ]
    }
   ],
   "source": [
    "PATH_USER_DICT = os.path.join(hub, \"user_dict.npy\")\n",
    "PATH_EVAL_DATSET = os.path.join(hub, \"eval_dict.npy\")\n",
    "PATH_USER_HISTORY_LENS = os.path.join(hub, 'users_history_len.npy')\n",
    "PATH_DATA_NUMBER = os.path.join(hub, \"data_number.npy\")\n",
    "\n",
    "users_dict = np.load(PATH_USER_DICT,allow_pickle='TRUE').item()\n",
    "eval_users_dict = np.load(PATH_EVAL_DATSET,allow_pickle='TRUE').item()\n",
    "data_number = np.load(PATH_DATA_NUMBER,allow_pickle='TRUE').item()\n",
    "users_history_lens = np.load(PATH_USER_HISTORY_LENS, allow_pickle='TRUE')\n",
    "all_items = {data[0] for i, k in users_dict.items() for data in k}  ## list video toan tap data \n",
    "\n",
    "user_dataset = eval_users_dict\n",
    "user_id = 4834\n",
    "\n",
    "users_history_lens = round(len(user_dataset[user_id]) * 0.6) ## split data in EVAL into 2 pieces: one for history and other for streaming \n",
    "watched_videos = [data[0] for data in user_dataset[user_id]][:users_history_lens]\n",
    "user_dict_history = {user_id: eval_users_dict[user_id][:users_history_lens]}\n",
    "# newest_watched_video = np.random.choice([i[0] for i in user_dataset[user_id]])\n",
    "newest_watched_video = [data[0] for data in user_dataset[user_id]][users_history_lens:][0]\n",
    "items_ids = np.array(list(set(all_items) - set(watched_videos)))\n",
    "len_items_ids = len(items_ids)\n",
    "users_num = data_number['users_num']\n",
    "items_num = data_number['items_num']\n",
    "\n",
    "STATE_SIZE =  5 #len_items_ids ## 1445 là số lượng videos sau khi trừ đi các video đã xem trong history\n",
    "num_actions =  5 #len_items_ids ## Number of list video to be choosed\n",
    "output_dim = 5 #len_items_ids\n",
    "\n",
    "env_prod = StimulateEnv(user_id, newest_watched_video, user_dict_history, users_history_lens, STATE_SIZE)\n",
    "recommender  = DDPG(env_prod, users_num, items_num, num_actions, STATE_SIZE, output_dim)  # output_dim là output của State_emebedding, để 1445 vì đầu vào của actor.evaluate_actor là (1445,400)\n",
    "\n",
    "\n",
    "EMBEDDING_SIZE = 100\n",
    "epsilon_for_priority = 1e-6\n",
    "batch_size = 32\n",
    "exploration_noise = OUNoise(num_actions)\n",
    "\n",
    "## item_ids should comes from history of user \n",
    "\n",
    "# x = items_ids\n",
    "x = watched_videos[- STATE_SIZE:]\n",
    "array_x = np.reshape(x,[1, num_actions])\n",
    "state_value = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "state_value = tf.expand_dims(state_value, axis=0)\n",
    "\n",
    "action = recommender.evaluate_actor(state_value) ## create a ranking video weight \n",
    "noise = exploration_noise.noise()\n",
    "action = action[0] + noise\n",
    "\n",
    "recommended_item = recommender.recommend_item(action, all_items, env_prod.old_watched, top_k= 5) ## create a list of suggested video \n",
    "next_items_ids_embs, reward, done, _ = env_prod.step(recommended_item)\n",
    "\n",
    "array_x_next = np.reshape(next_items_ids_embs,[1, num_actions])\n",
    "state_value_next = tf.convert_to_tensor(next_items_ids_embs, dtype=tf.float32)\n",
    "state_value_next = tf.expand_dims(state_value_next, axis=0)\n",
    "\n",
    "reward = np.sum(reward)\n",
    "\n",
    "recommender.add_experience(state_value, state_value_next, action, reward, done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note flow \n",
    "\n",
    "reset will take out 5 videos from history of user, 5 videos \n",
    "        -> embedding -> state -> actor.network \n",
    "        -> action (ranking video base on user) -> feed on recommender.recommend_item (take out video to suggest) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1135, 3515, 1434, 2178, 1544]),\n",
       " [2395, 2762, 2724, 314, 2702],\n",
       " 2840,\n",
       " [2395, 2762, 2724, 314, 2702])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_item, env_prod.old_watched, newest_watched_video, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/9: finish flow evaluate_actor and predict\n",
    "\n",
    "next step: replay_buffer and train sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_USER_DICT = os.path.join(hub, \"user_dict.npy\")\n",
    "PATH_EVAL_DATSET = os.path.join(hub, \"eval_dict.npy\")\n",
    "PATH_USER_HISTORY_LENS = os.path.join(hub, 'users_history_len.npy')\n",
    "PATH_DATA_NUMBER = os.path.join(hub, \"data_number.npy\")\n",
    "\n",
    "users_dict = np.load(PATH_USER_DICT,allow_pickle='TRUE').item()\n",
    "eval_users_dict = np.load(PATH_EVAL_DATSET,allow_pickle='TRUE').item()\n",
    "data_number = np.load(PATH_DATA_NUMBER,allow_pickle='TRUE').item()\n",
    "users_history_lens = np.load(PATH_USER_HISTORY_LENS, allow_pickle='TRUE')\n",
    "all_items = {data[0] for i, k in users_dict.items() for data in k}  ## list video toan tap data \n",
    "\n",
    "user_dataset = eval_users_dict\n",
    "users_num = data_number['users_num']\n",
    "items_num = data_number['items_num']\n",
    "\n",
    "STATE_SIZE = 20\n",
    "num_actions = 20\n",
    "output_dim = 5\n",
    "\n",
    "user_id = 4833\n",
    "users_history_lens = round(len(eval_users_dict[user_id]) * 0.6)\n",
    "watched_videos = [video[0] for video in eval_users_dict[user_id]][:users_history_lens]\n",
    "newest_watched_video = [video[0] for video in eval_users_dict[user_id]][users_history_lens:][0]\n",
    "enviroment = StimulateEnv(user_id, newest_watched_video, eval_users_dict, users_history_lens,STATE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2908,\n",
       " 3175,\n",
       " 3007,\n",
       " 2959,\n",
       " 3006,\n",
       " 2762,\n",
       " 2997,\n",
       " 3052,\n",
       " 3125,\n",
       " 2912,\n",
       " 2712,\n",
       " 3053,\n",
       " 2976,\n",
       " 3005,\n",
       " 2844,\n",
       " 2893,\n",
       " 3051,\n",
       " 3185,\n",
       " 3238,\n",
       " 3113]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data[0] for data in eval_users_dict[user_id][-20:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.embedding.Embedding object at 0x16cc0f430>. Skipping object. Exception encountered: Layer 'user_embedding' expected 0 variables, but received 1 variables during loading. Expected: []\n",
      "  _load_state(\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.embedding.Embedding object at 0x16cc0c8b0>. Skipping object. Exception encountered: Layer 'video_embedding' expected 0 variables, but received 1 variables during loading. Expected: []\n",
      "  _load_state(\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.dense.Dense object at 0x16cbc0670>. Skipping object. Exception encountered: Layer 'dense' expected 0 variables, but received 2 variables during loading. Expected: []\n",
      "  _load_state(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Starting episode no: 0 ==== \n",
      "\n",
      "history to embs: [1729, 1230, 1128, 780, 1193, 1222, 3328, 3536, 3481, 3571, 3408, 3578, 3624, 3083, 527, 3518, 318, 912, 296, 608]\n",
      "env items: None\n",
      "suggested items: [3272 1336  173 3859  776 2877 2700 2150 2193 2638 1285 3264    2 1395\n",
      "  569 2979  818 3242 1848  828]\n",
      "actually watch 1196\n",
      "next items to embs: None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext items to embs:\u001b[39m\u001b[38;5;124m\"\u001b[39m, next_items_ids_embs)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# print(newest_watched_video_, recommended_item, next_items_ids_embs)\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m state_value_next \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_items_ids_embs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m state_value_next \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(state_value_next, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     54\u001b[0m state_value_next \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(state_value_next,[\u001b[38;5;241m1\u001b[39m, num_actions])\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/hanoi/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "\n",
    "#specify parameters here:\n",
    "episodes=1\n",
    "\n",
    "#Randomly initialize critic,actor,target critic, target actor network  and replay buffer   \n",
    "# agent  = DDPG(enviroment, users_num, items_num, num_actions, STATE_SIZE, output_dim)  # output_dim là output của State_emebedding, để 1445 vì đầu vào của actor.evaluate_actor là (1445,400)\n",
    "exploration_noise = OUNoise(num_actions)\n",
    "counter=0\n",
    "reward_per_episode = 0    \n",
    "total_reward=0\n",
    "#saving reward:\n",
    "reward_st = np.array([0])\n",
    "    \n",
    "agent  = DDPG(enviroment, users_num, items_num, num_actions, STATE_SIZE, output_dim)  # output_dim là output của State_emebedding, để 1445 vì đầu vào của actor.evaluate_actor là (1445,400)\n",
    "\n",
    "for i in range(0, episodes):\n",
    "    print(\"==== Starting episode no:\",i,\"====\",\"\\n\")\n",
    "    user_id, watched_videos, done = enviroment.reset()\n",
    "\n",
    "    # user_id = 4833\n",
    "    # users_history_lens = round(len(eval_users_dict[user_id]) * 0.6)\n",
    "    users_history_lens = STATE_SIZE\n",
    "    eval_users_dict_stimulated = {user_id: eval_users_dict[user_id][:users_history_lens]}\n",
    "    watched_videos = [video[0] for video in eval_users_dict[user_id]][:users_history_lens]\n",
    "    newest_watched_video = [video[0] for video in eval_users_dict[user_id]][users_history_lens:]\n",
    "    old_watched = enviroment.old_watched\n",
    "    newest_watched_video_ = newest_watched_video[i] ## initiate video to start a session\n",
    "\n",
    "    # users_history_lens =  round(len(user_dataset[user_id]) * 0.6)\n",
    "    # watched_videos =  [data[0] for data in eval_users_dict[user_id]][:users_history_lens]\n",
    "    reward_per_episode = 0\n",
    "    steps = len(eval_users_dict[user_id][users_history_lens:])\n",
    "    \n",
    "    for t in range(0, steps-1):\n",
    "        x = watched_videos[- STATE_SIZE:]\n",
    "        ## change shape to fit evaluate_actor \n",
    "        state_value = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "        state_value = tf.expand_dims(state_value, axis=0)\n",
    "        action = agent.evaluate_actor(np.reshape(state_value,[1, num_actions]))\n",
    "        noise = exploration_noise.noise()\n",
    "        action = action[0] + noise #Select action according to current policy and exploration noise\n",
    "        # print(\"Action at step\", t ,\" :\",action,\"\\n\")\n",
    "        enviroment = StimulateEnv(user_id, newest_watched_video_, eval_users_dict_stimulated, users_history_lens, STATE_SIZE)\n",
    "        recommended_item = agent.recommend_item(action, all_items, old_watched, top_k= STATE_SIZE)\n",
    "\n",
    "        next_items_ids_embs, reward, done, _= enviroment.step(recommended_item)\n",
    "        print(\"history to embs:\", x)\n",
    "        print(\"env items:\", enviroment.items)\n",
    "        print(\"suggested items:\", recommended_item)\n",
    "        print(\"actually watch\", newest_watched_video_)\n",
    "        print(\"next items to embs:\", next_items_ids_embs)\n",
    "        # print(newest_watched_video_, recommended_item, next_items_ids_embs)\n",
    "        state_value_next = tf.convert_to_tensor(next_items_ids_embs, dtype=tf.float32)\n",
    "        state_value_next = tf.expand_dims(state_value_next, axis=0)\n",
    "        state_value_next = np.reshape(state_value_next,[1, num_actions])\n",
    "        \n",
    "        newest_watched_video_ = newest_watched_video[t+1] ## giả lập truyền liên tục vào Enviroment các video mới cho lần kế tiếp\n",
    "        old_watched = np.append(old_watched, enviroment.newest_watched_video)\n",
    "        list_watched_old = eval_users_dict_stimulated[user_id]\n",
    "        list_watched_old.append((enviroment.newest_watched_video, 5))\n",
    "        eval_users_dict_stimulated = {user_id: list_watched_old} \n",
    "        watched_videos.append(newest_watched_video[t])\n",
    "        \n",
    "        #add s_t,s_t+1,action,reward to experience memory\n",
    "        agent.add_experience(state_value, state_value_next, action, reward, done)\n",
    "        #train critic and actor network\n",
    "        if counter > 30: \n",
    "            agent.train()\n",
    "        reward_per_episode+=reward[0]\n",
    "        counter+=1\n",
    "        #check if episode ends:\n",
    "        if (done or (t == steps-1) or t == 30):\n",
    "            print('EPISODE: ',i,' Steps: ',t,' Total Reward: ',reward_per_episode)\n",
    "            print(\"Printing reward to file\")\n",
    "            exploration_noise.reset() #reinitializing random noise for action exploration\n",
    "            reward_st = np.append(reward_st,reward_per_episode)\n",
    "            np.savetxt('episode_reward.txt',reward_st, newline=\"\\n\")\n",
    "            print('\\n\\n')\n",
    "            break\n",
    "total_reward+=reward_per_episode            \n",
    "print(\"Average reward per episode {}\".format(total_reward / episodes)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stuck: next_state in replay_memory not work properly vì hiện tại đang lấy là các videos từ lịch sử tiếp theo. Nghiên cứu lại vấn đề next_state\n",
    "\n",
    "Hiện tại đang là, history -> embedding state (neural of actor): action -> nhân action * list video tổng (đã bỏ qua history) -> weight các videos -> Lấy 20 video có weight tốt nhát -> compare with thực tế -> lấy thông tin next_state từ history -> Lưu vào trong replay_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(480, 4),\n",
       "  (2269, 2),\n",
       "  (1210, 5),\n",
       "  (2090, 4),\n",
       "  (1586, 1),\n",
       "  (3208, 2),\n",
       "  (593, 4),\n",
       "  (527, 5),\n",
       "  (1704, 3),\n",
       "  (608, 1),\n",
       "  (3006, 3),\n",
       "  (1617, 4),\n",
       "  (457, 5),\n",
       "  (17, 4),\n",
       "  (2571, 5),\n",
       "  (497, 4),\n",
       "  (150, 5),\n",
       "  (590, 3),\n",
       "  (1921, 1),\n",
       "  (2268, 4),\n",
       "  (2336, 3),\n",
       "  (553, 4),\n",
       "  (3476, 2),\n",
       "  (508, 4),\n",
       "  (356, 4),\n",
       "  (34, 4),\n",
       "  (2501, 5),\n",
       "  (588, 4),\n",
       "  (2916, 5),\n",
       "  (2353, 5),\n",
       "  (2355, 4),\n",
       "  (1641, 3),\n",
       "  (1909, 4),\n",
       "  (2617, 5),\n",
       "  (1198, 5),\n",
       "  (266, 2)],\n",
       " array([2353, 2355, 1641, 1909, 2617,  527,  593,  527, 1704,  608]),\n",
       " 3006,\n",
       " [(480, 4),\n",
       "  (2269, 2),\n",
       "  (1210, 5),\n",
       "  (2090, 4),\n",
       "  (1586, 1),\n",
       "  (527, 5),\n",
       "  (593, 5),\n",
       "  (527, 5),\n",
       "  (1704, 5),\n",
       "  (608, 5)],\n",
       " [480, 2269, 1210, 2090, 1586, 3208, 593, 527, 1704, 608])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_users_dict[user_id], old_watched, newest_watched_video_, eval_users_dict_stimulated[user_id], watched_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3] \n",
    "a.append(4)\n",
    "a\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hanoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
