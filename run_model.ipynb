{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.ops.gen_math_ops import Exp\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import sys \n",
    "\n",
    "from model.actor import Actor\n",
    "from model.critic import Critic\n",
    "from model.enviroment import StimulateEnv\n",
    "from model.ddpg import DDPG \n",
    "from model.embedding import VideoGenreEmbedding, UserVideoEmbedding\n",
    "from model.ou_noise import OUNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub = '/Users/minhtuan/Documents/Documents/Work/Hanoi/Hub'\n",
    "path = '/Users/minhtuan/Documents/Documents/Work/Hanoi/crawler/X/movielens/airflow_folder/drl_melody/storage/dataset1M'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.embedding.Embedding object at 0x17755e320>. Skipping object. Exception encountered: Layer 'user_embedding' expected 0 variables, but received 1 variables during loading. Expected: []\n",
      "  _load_state(\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.embedding.Embedding object at 0x17755df60>. Skipping object. Exception encountered: Layer 'video_embedding' expected 0 variables, but received 1 variables during loading. Expected: []\n",
      "  _load_state(\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.dense.Dense object at 0x17755d990>. Skipping object. Exception encountered: Layer 'dense_4' expected 0 variables, but received 2 variables during loading. Expected: []\n",
      "  _load_state(\n"
     ]
    }
   ],
   "source": [
    "PATH_USER_DICT = os.path.join(hub, \"user_dict.npy\")\n",
    "PATH_EVAL_DATSET = os.path.join(hub, \"eval_dict.npy\")\n",
    "PATH_USER_HISTORY_LENS = os.path.join(hub, 'users_history_len.npy')\n",
    "PATH_DATA_NUMBER = os.path.join(hub, \"data_number.npy\")\n",
    "\n",
    "users_dict = np.load(PATH_USER_DICT,allow_pickle='TRUE').item()\n",
    "eval_users_dict = np.load(PATH_EVAL_DATSET,allow_pickle='TRUE').item()\n",
    "data_number = np.load(PATH_DATA_NUMBER,allow_pickle='TRUE').item()\n",
    "users_history_lens = np.load(PATH_USER_HISTORY_LENS, allow_pickle='TRUE')\n",
    "all_items = {data[0] for i, k in users_dict.items() for data in k}  ## list video toan tap data \n",
    "\n",
    "user_dataset = eval_users_dict\n",
    "user_id = 4834\n",
    "\n",
    "users_history_lens = round(len(user_dataset[user_id]) * 0.6) ## split data in EVAL into 2 pieces: one for history and other for streaming \n",
    "watched_videos = [data[0] for data in user_dataset[user_id]][:users_history_lens]\n",
    "user_dict_history = {user_id: eval_users_dict[user_id][:users_history_lens]}\n",
    "# newest_watched_video = np.random.choice([i[0] for i in user_dataset[user_id]])\n",
    "newest_watched_video = [data[0] for data in user_dataset[user_id]][users_history_lens:][0]\n",
    "items_ids = np.array(list(set(all_items) - set(watched_videos)))\n",
    "len_items_ids = len(items_ids)\n",
    "users_num = data_number['users_num']\n",
    "items_num = data_number['items_num']\n",
    "\n",
    "STATE_SIZE =  5 #len_items_ids ## 1445 là số lượng videos sau khi trừ đi các video đã xem trong history\n",
    "num_actions =  5 #len_items_ids ## Number of list video to be choosed\n",
    "output_dim = 5 #len_items_ids\n",
    "\n",
    "env_prod = StimulateEnv(user_id, newest_watched_video, user_dict_history, users_history_lens, STATE_SIZE)\n",
    "recommender  = DDPG(env_prod, users_num, items_num, num_actions, STATE_SIZE, output_dim)  # output_dim là output của State_emebedding, để 1445 vì đầu vào của actor.evaluate_actor là (1445,400)\n",
    "\n",
    "\n",
    "EMBEDDING_SIZE = 100\n",
    "epsilon_for_priority = 1e-6\n",
    "batch_size = 32\n",
    "exploration_noise = OUNoise(num_actions)\n",
    "\n",
    "## item_ids should comes from history of user \n",
    "\n",
    "# x = items_ids\n",
    "x = watched_videos[- STATE_SIZE:]\n",
    "array_x = np.reshape(x,[1, num_actions])\n",
    "state_value = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "state_value = tf.expand_dims(state_value, axis=0)\n",
    "\n",
    "action = recommender.evaluate_actor(state_value) ## create a ranking video weight \n",
    "noise = exploration_noise.noise()\n",
    "action = action[0] + noise\n",
    "\n",
    "recommended_item = recommender.recommend_item(action, all_items, env_prod.old_watched, top_k= 5) ## create a list of suggested video \n",
    "next_items_ids_embs, reward, done, _ = env_prod.step(recommended_item)\n",
    "\n",
    "array_x_next = np.reshape(next_items_ids_embs,[1, num_actions])\n",
    "state_value_next = tf.convert_to_tensor(next_items_ids_embs, dtype=tf.float32)\n",
    "state_value_next = tf.expand_dims(state_value_next, axis=0)\n",
    "\n",
    "reward = np.sum(reward)\n",
    "\n",
    "recommender.add_experience(state_value, state_value_next, action, reward, done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note flow \n",
    "\n",
    "reset will take out 5 videos from history of user, 5 videos \n",
    "        -> embedding -> state -> actor.network \n",
    "        -> action (ranking video base on user) -> feed on recommender.recommend_item (take out video to suggest) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1135, 3515, 1434, 2178, 1544]),\n",
       " [2395, 2762, 2724, 314, 2702],\n",
       " 2840,\n",
       " [2395, 2762, 2724, 314, 2702])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_item, env_prod.old_watched, newest_watched_video, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/9: finish flow evaluate_actor and predict\n",
    "\n",
    "next step: replay_buffer and train sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_USER_DICT = os.path.join(hub, \"user_dict.npy\")\n",
    "PATH_EVAL_DATSET = os.path.join(hub, \"eval_dict.npy\")\n",
    "PATH_USER_HISTORY_LENS = os.path.join(hub, 'users_history_len.npy')\n",
    "PATH_DATA_NUMBER = os.path.join(hub, \"data_number.npy\")\n",
    "\n",
    "users_dict = np.load(PATH_USER_DICT,allow_pickle='TRUE').item()\n",
    "eval_users_dict = np.load(PATH_EVAL_DATSET,allow_pickle='TRUE').item()\n",
    "data_number = np.load(PATH_DATA_NUMBER,allow_pickle='TRUE').item()\n",
    "users_history_lens = np.load(PATH_USER_HISTORY_LENS, allow_pickle='TRUE')\n",
    "all_items = {data[0] for i, k in users_dict.items() for data in k}  ## list video toan tap data \n",
    "\n",
    "user_dataset = eval_users_dict\n",
    "users_num = data_number['users_num']\n",
    "items_num = data_number['items_num']\n",
    "\n",
    "STATE_SIZE = 20\n",
    "num_actions = 20\n",
    "output_dim = 5\n",
    "\n",
    "user_id = 4833\n",
    "users_history_lens = round(len(eval_users_dict[user_id]) * 0.6)\n",
    "watched_videos = [video[0] for video in eval_users_dict[user_id]][:users_history_lens]\n",
    "newest_watched_video = [video[0] for video in eval_users_dict[user_id]][users_history_lens:][0]\n",
    "enviroment = StimulateEnv(user_id, newest_watched_video, eval_users_dict, users_history_lens,STATE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.embedding.Embedding object at 0x1761f7b80>. Skipping object. Exception encountered: Layer 'user_embedding' expected 0 variables, but received 1 variables during loading. Expected: []\n",
      "  _load_state(\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.embedding.Embedding object at 0x1761f5c90>. Skipping object. Exception encountered: Layer 'video_embedding' expected 0 variables, but received 1 variables during loading. Expected: []\n",
      "  _load_state(\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.dense.Dense object at 0x1761f4040>. Skipping object. Exception encountered: Layer 'dense_12' expected 0 variables, but received 2 variables during loading. Expected: []\n",
      "  _load_state(\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.embedding.Embedding object at 0x17626cf40>. Skipping object. Exception encountered: Layer 'user_embedding' expected 0 variables, but received 1 variables during loading. Expected: []\n",
      "  _load_state(\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.embedding.Embedding object at 0x17626d330>. Skipping object. Exception encountered: Layer 'video_embedding' expected 0 variables, but received 1 variables during loading. Expected: []\n",
      "  _load_state(\n",
      "/Users/minhtuan/anaconda3/envs/hanoi/lib/python3.10/site-packages/keras/saving/saving_lib.py:510: UserWarning: Could not load weights in object <keras.layers.core.dense.Dense object at 0x17626d1e0>. Skipping object. Exception encountered: Layer 'dense_14' expected 0 variables, but received 2 variables during loading. Expected: []\n",
      "  _load_state(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Starting episode no: 0 ==== \n",
      "\n",
      "[   1    2    3 ... 3950 3951 3952]\n",
      "history to embs: [1929, 3108, 10, 296, 858, 3503, 1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303]\n",
      "env items: [1929, 3108, 10, 296, 858, 3503, 1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303]\n",
      "suggested items: [2589  588 3665  943   17 2081 1080 3249 2661 3801 3868  128 2534 1911\n",
      " 3540 1814  717 1599 2865 3932]\n",
      "actually watch 1084\n",
      "next items to embs: [1929, 3108, 10, 296, 858, 3503, 1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303]\n",
      "[   1    2    3 ... 3950 3951 3952]\n",
      "history to embs: [3108, 10, 296, 858, 3503, 1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084]\n",
      "env items: [3108, 10, 296, 858, 3503, 1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084]\n",
      "suggested items: [1951 2017 1468  600 3370 2534 2658 2381 3098 2143 2131 1701  892 2523\n",
      " 1015 2271 2981 1222 3254 1342]\n",
      "actually watch 2075\n",
      "next items to embs: [3108, 10, 296, 858, 3503, 1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084]\n",
      "[   1    2    3 ... 3950 3951 3952]\n",
      "history to embs: [10, 296, 858, 3503, 1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075]\n",
      "env items: [10, 296, 858, 3503, 1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075]\n",
      "suggested items: [2592 1098 1016 3636 1252  359 1500 2142  600 3773 3634   58 2488 2963\n",
      " 3051 2472 3537  224 2534 3254]\n",
      "actually watch 1406\n",
      "next items to embs: [10, 296, 858, 3503, 1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075]\n",
      "[   1    2    3 ... 3950 3951 3952]\n",
      "history to embs: [296, 858, 3503, 1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406]\n",
      "env items: [296, 858, 3503, 1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406]\n",
      "suggested items: [3098 1721 1460 2592 1859 3365 3600  658 1404 3535 2658 3762 2503  600\n",
      " 1500 1215 3537 1098 1701 3588]\n",
      "actually watch 3095\n",
      "next items to embs: [296, 858, 3503, 1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406]\n",
      "[   1    2    3 ... 3950 3951 3952]\n",
      "history to embs: [858, 3503, 1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095]\n",
      "env items: [858, 3503, 1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095]\n",
      "suggested items: [2169 2592  186  974 1433 1477 1293  839 2142 2658  990 1672 2765  642\n",
      " 2503 3588  852 3291 1916 1215]\n",
      "actually watch 670\n",
      "next items to embs: [858, 3503, 1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095]\n",
      "[   1    2    3 ... 3950 3951 3952]\n",
      "history to embs: [3503, 1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670]\n",
      "env items: [3503, 1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670]\n",
      "suggested items: [1916 1500 2095  366 3111 1215 2521 2433 1307 1455  507 2142  267  852\n",
      "  186 1016  564 3588 3291  990]\n",
      "actually watch 2732\n",
      "next items to embs: [3503, 1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670]\n",
      "[   1    2    3 ... 3950 3951 3952]\n",
      "history to embs: [1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732]\n",
      "env items: [1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732]\n",
      "suggested items: [ 778  305 2429 1683 1815 2092 1455 3588  355 1394 2737  996 2643  737\n",
      "  445   65 1369 3111 1500 2762]\n",
      "actually watch 111\n",
      "next items to embs: [1221, 3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732]\n",
      "[   1    2    3 ... 3950 3951 3952]\n",
      "history to embs: [3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111]\n",
      "env items: [3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111]\n",
      "suggested items: [3820 2429  940 2800 2311 3762 3060 2362 3651  299 2092  396 2923 3886\n",
      " 1793 2762 1500  355 1690   65]\n",
      "actually watch 3093\n",
      "next items to embs: [3307, 2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111]\n",
      "[   1    2    3 ... 3950 3951 3952]\n",
      "history to embs: [2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093]\n",
      "env items: [2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093]\n",
      "suggested items: [3060  700 2175  889  884 2762 1840 3886 3651 2557  396 3781 2092 2875\n",
      "   65  154 2362 3620  299 1526]\n",
      "actually watch 926\n",
      "next items to embs: [2858, 1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093]\n",
      "[   1    2    3 ... 3950 3951 3952]\n",
      "history to embs: [1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926]\n",
      "env items: [1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926]\n",
      "suggested items: [2762 3620 2501 3442  355 3129 1959 2362 2375 1793 1500 2313 1526 2335\n",
      " 3651 3762 3104  396  299 3702]\n",
      "actually watch 919\n",
      "next items to embs: [1193, 912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926]\n",
      "[   1    2    3 ... 3950 3951 3952]\n",
      "history to embs: [912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919]\n",
      "env items: [912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919]\n",
      "suggested items: [3251 2156 3218 2180 3104 2762 2092 1840  755 1250 1690 3702 1224 1344\n",
      "  396 3651 2620 2263  299 2313]\n",
      "actually watch 1219\n",
      "next items to embs: [912, 923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919]\n",
      "[   1    2    3 ... 3950 3951 3952]\n",
      "history to embs: [923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219]\n",
      "env items: [923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219]\n",
      "suggested items: [ 267 3651 2837 2175 3104 1412 2156 3702 1224 2620 2509 3218  396  299\n",
      " 2762 3382 2180 2263 2313 2335]\n",
      "actually watch 2648\n",
      "next items to embs: [923, 2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219]\n",
      "[   1    2    3 ... 3950 3951 3952]\n",
      "history to embs: [2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648]\n",
      "env items: [2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648]\n",
      "suggested items: [ 167 2837  207 2924 3434 3805  978 2901 2620  666 1344 2311 3026 2834\n",
      " 1224 2180 2156 1412 2263 2335]\n",
      "actually watch 2160\n",
      "next items to embs: [2019, 3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648]\n",
      "[   1    2    3 ... 3950 3951 3952]\n",
      "history to embs: [3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648, 2160]\n",
      "env items: [3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648, 2160]\n",
      "suggested items: [ 252 1284 1793  708 2006  659 3104 1412 2762 1690 1344 3629 1224 3653\n",
      "  396 2335 2313 2263 2620 2180]\n",
      "actually watch 2784\n",
      "next items to embs: [3134, 3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648, 2160]\n",
      "[   1    2    3 ... 3950 3951 3952]\n",
      "history to embs: [3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648, 2160, 2784]\n",
      "env items: [3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648, 2160, 2784]\n",
      "suggested items: [1793 3822 3382 2853 2156 3629 2876  708 3653 3651 1690 1344 3104 3689\n",
      " 2762  396 3702 2620 2263 2313]\n",
      "actually watch 2644\n",
      "next items to embs: [3224, 1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648, 2160, 2784]\n",
      "[   1    2    3 ... 3950 3951 3952]\n",
      "history to embs: [1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648, 2160, 2784, 2644]\n",
      "env items: [1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648, 2160, 2784, 2644]\n",
      "suggested items: [2038 3822 2375  160    6  299 3689 3382 2174 1250 2620  583 1933 2175\n",
      " 3467 2313 2157 2824 2263 3702]\n",
      "actually watch 1350\n",
      "next items to embs: [1207, 1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648, 2160, 2784, 2644]\n",
      "[   1    2    3 ... 3950 3951 3952]\n",
      "history to embs: [1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648, 2160, 2784, 2644, 1350]\n",
      "env items: [1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648, 2160, 2784, 2644, 1350]\n",
      "suggested items: [1961 3837 2620 2647 2263  273 3723 1690  396 3845 2375 2313 1126  421\n",
      " 1483 2762 3689 1004 1822 3702]\n",
      "actually watch 3499\n",
      "next items to embs: [1237, 3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648, 2160, 2784, 2644, 1350]\n",
      "[   1    2    3 ... 3950 3951 3952]\n",
      "history to embs: [3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648, 2160, 2784, 2644, 1350, 3499]\n",
      "env items: [3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648, 2160, 2784, 2644, 1350, 3499]\n",
      "suggested items: [ 893 1944 3565   42 3822  898 3182 2313  196 3702 3568  767 2716 2210\n",
      " 2156 3557 2587  496 3689  583]\n",
      "actually watch 1253\n",
      "next items to embs: [3030, 908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648, 2160, 2784, 2644, 1350, 3499]\n",
      "[   1    2    3 ... 3950 3951 3952]\n",
      "history to embs: [908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648, 2160, 2784, 2644, 1350, 3499, 1253]\n",
      "env items: [908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648, 2160, 2784, 2644, 1350, 3499, 1253]\n",
      "suggested items: [1934  368 2904 2210  898 2399 1126  421 3565 1702 1711 3689 1022  496\n",
      " 2647 3723   42  224  829 1954]\n",
      "actually watch 260\n",
      "next items to embs: [908, 2303, 1084, 2075, 1406, 3095, 670, 2732, 111, 3093, 926, 919, 1219, 2648, 2160, 2784, 2644, 1350, 3499, 1253]\n",
      "Average reward per episode -9.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#specify parameters here:\n",
    "episodes=1\n",
    "\n",
    "#Randomly initialize critic,actor,target critic, target actor network  and replay buffer   \n",
    "agent  = DDPG(enviroment, users_num, items_num, num_actions, STATE_SIZE, output_dim)  # output_dim là output của State_emebedding, để 1445 vì đầu vào của actor.evaluate_actor là (1445,400)\n",
    "exploration_noise = OUNoise(num_actions)\n",
    "counter=0\n",
    "reward_per_episode = 0    \n",
    "total_reward=0\n",
    "#saving reward:\n",
    "reward_st = np.array([0])\n",
    "    \n",
    "agent  = DDPG(enviroment, users_num, items_num, num_actions, STATE_SIZE, output_dim)  # output_dim là output của State_emebedding, để 1445 vì đầu vào của actor.evaluate_actor là (1445,400)\n",
    "\n",
    "for i in range(0, episodes):\n",
    "    print(\"==== Starting episode no:\",i,\"====\",\"\\n\")\n",
    "    user_id, watched_videos, done = enviroment.reset()\n",
    "\n",
    "    # user_id = 4833\n",
    "    # users_history_lens = round(len(eval_users_dict[user_id]) * 0.6)\n",
    "    users_history_lens = STATE_SIZE\n",
    "    eval_users_dict_stimulated = {user_id: eval_users_dict[user_id][:users_history_lens]}\n",
    "    watched_videos = [video[0] for video in eval_users_dict[user_id]][:users_history_lens]\n",
    "    newest_watched_video = [video[0] for video in eval_users_dict[user_id]][users_history_lens:]\n",
    "    old_watched = enviroment.old_watched\n",
    "    newest_watched_video_ = newest_watched_video[i] ## initiate video to start a session\n",
    "\n",
    "    # users_history_lens =  round(len(user_dataset[user_id]) * 0.6)\n",
    "    # watched_videos =  [data[0] for data in eval_users_dict[user_id]][:users_history_lens]\n",
    "    reward_per_episode = 0\n",
    "    steps = len(eval_users_dict[user_id][users_history_lens:])\n",
    "    for t in range(0, steps-1):\n",
    "        x = watched_videos[- STATE_SIZE:]\n",
    "        ## change shape to fit evaluate_actor \n",
    "        state_value = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "        state_value = tf.expand_dims(state_value, axis=0)\n",
    "        action = agent.evaluate_actor(np.reshape(state_value,[1, num_actions]))\n",
    "        noise = exploration_noise.noise()\n",
    "        action = action[0] + noise #Select action according to current policy and exploration noise\n",
    "        # print(\"Action at step\", t ,\" :\",action,\"\\n\")\n",
    "        enviroment = StimulateEnv(user_id, newest_watched_video_, eval_users_dict_stimulated, users_history_lens, STATE_SIZE)\n",
    "        recommended_item = agent.recommend_item(action, all_items, old_watched, top_k= STATE_SIZE)\n",
    "\n",
    "        next_items_ids_embs, reward, done, _= enviroment.step(recommended_item)\n",
    "        print(\"history to embs:\", x)\n",
    "        print(\"env items:\", enviroment.items)\n",
    "        print(\"suggested items:\", recommended_item)\n",
    "        print(\"actually watch\", newest_watched_video_)\n",
    "        print(\"next items to embs:\", next_items_ids_embs)\n",
    "        # print(newest_watched_video_, recommended_item, next_items_ids_embs)\n",
    "        state_value_next = tf.convert_to_tensor(next_items_ids_embs, dtype=tf.float32)\n",
    "        state_value_next = tf.expand_dims(state_value_next, axis=0)\n",
    "        state_value_next = np.reshape(state_value_next,[1, num_actions])\n",
    "        \n",
    "        newest_watched_video_ = newest_watched_video[t+1] ## giả lập truyền liên tục vào Enviroment các video mới cho lần kế tiếp\n",
    "        old_watched = np.append(old_watched, enviroment.newest_watched_video)\n",
    "        list_watched_old = eval_users_dict_stimulated[user_id]\n",
    "        list_watched_old.append((enviroment.newest_watched_video, 5))\n",
    "        eval_users_dict_stimulated = {user_id: list_watched_old} \n",
    "        watched_videos.append(newest_watched_video[t])\n",
    "        \n",
    "        #add s_t,s_t+1,action,reward to experience memory\n",
    "        agent.add_experience(state_value, state_value_next, action, reward, done)\n",
    "        #train critic and actor network\n",
    "        if counter > 30: \n",
    "            agent.train()\n",
    "        reward_per_episode+=reward[0]\n",
    "        counter+=1\n",
    "        #check if episode ends:\n",
    "        if (done or (t == steps-1) or t == 30):\n",
    "            print('EPISODE: ',i,' Steps: ',t,' Total Reward: ',reward_per_episode)\n",
    "            print(\"Printing reward to file\")\n",
    "            exploration_noise.reset() #reinitializing random noise for action exploration\n",
    "            reward_st = np.append(reward_st,reward_per_episode)\n",
    "            np.savetxt('episode_reward.txt',reward_st, newline=\"\\n\")\n",
    "            print('\\n\\n')\n",
    "            break\n",
    "total_reward+=reward_per_episode            \n",
    "print(\"Average reward per episode {}\".format(total_reward / episodes)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stuck: next_state in replay_memory not work properly vì hiện tại đang lấy là các videos từ lịch sử tiếp theo. Nghiên cứu lại vấn đề next_state\n",
    "\n",
    "Hiện tại đang là, history -> embedding state (neural of actor): action -> nhân action * list video tổng (đã bỏ qua history) -> weight các videos -> Lấy 20 video có weight tốt nhát -> compare with thực tế -> lấy thông tin next_state từ history -> Lưu vào trong replay_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(480, 4),\n",
       "  (2269, 2),\n",
       "  (1210, 5),\n",
       "  (2090, 4),\n",
       "  (1586, 1),\n",
       "  (3208, 2),\n",
       "  (593, 4),\n",
       "  (527, 5),\n",
       "  (1704, 3),\n",
       "  (608, 1),\n",
       "  (3006, 3),\n",
       "  (1617, 4),\n",
       "  (457, 5),\n",
       "  (17, 4),\n",
       "  (2571, 5),\n",
       "  (497, 4),\n",
       "  (150, 5),\n",
       "  (590, 3),\n",
       "  (1921, 1),\n",
       "  (2268, 4),\n",
       "  (2336, 3),\n",
       "  (553, 4),\n",
       "  (3476, 2),\n",
       "  (508, 4),\n",
       "  (356, 4),\n",
       "  (34, 4),\n",
       "  (2501, 5),\n",
       "  (588, 4),\n",
       "  (2916, 5),\n",
       "  (2353, 5),\n",
       "  (2355, 4),\n",
       "  (1641, 3),\n",
       "  (1909, 4),\n",
       "  (2617, 5),\n",
       "  (1198, 5),\n",
       "  (266, 2)],\n",
       " array([2353, 2355, 1641, 1909, 2617,  527,  593,  527, 1704,  608]),\n",
       " 3006,\n",
       " [(480, 4),\n",
       "  (2269, 2),\n",
       "  (1210, 5),\n",
       "  (2090, 4),\n",
       "  (1586, 1),\n",
       "  (527, 5),\n",
       "  (593, 5),\n",
       "  (527, 5),\n",
       "  (1704, 5),\n",
       "  (608, 5)],\n",
       " [480, 2269, 1210, 2090, 1586, 3208, 593, 527, 1704, 608])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_users_dict[user_id], old_watched, newest_watched_video_, eval_users_dict_stimulated[user_id], watched_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([(<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[3098.,  908., 2143., 1385.,  356.]], dtype=float32)>,\n",
       "        array([[3098.,  908., 2143., 1385.,  356.]], dtype=float32),\n",
       "        <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "        array([-0.13678792, -0.61636907, -0.32507613, -0.31683338,  0.03700582],\n",
       "              dtype=float32)>,\n",
       "        [-0.5],\n",
       "        False),\n",
       "       (<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[ 908., 2143., 1385.,  356., 1035.]], dtype=float32)>,\n",
       "        array([[ 908., 2143., 1385.,  356., 3526.]], dtype=float32),\n",
       "        <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "        array([-0.2489828 , -0.70440966, -0.30874968,  0.14436762, -0.02266764],\n",
       "              dtype=float32)>,\n",
       "        [-0.5],\n",
       "        False),\n",
       "       (<tf.Tensor: shape=(1, 5), dtype=float32, numpy=array([[2143., 1385.,  356., 1035., 3526.]], dtype=float32)>,\n",
       "        array([[2143., 1385.,  356., 3526., 3526.]], dtype=float32),\n",
       "        <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "        array([-0.68763566, -0.24136421, -0.3019367 ,  0.25600818,  0.22895858],\n",
       "              dtype=float32)>,\n",
       "        [-0.5],\n",
       "        False)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.replay_memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hanoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
